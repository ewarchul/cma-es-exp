\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
%EW:
\usepackage{float}
\usepackage{multirow}
%:EW
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{psfrag}

\usepackage{url}
\urldef{\mailsa}\path|jarabas@elka.pw.edu.pl|
\urldef{\mailsb}\path|ewarchul@mion.elka.pw.edu.pl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\newcommand{\wec}[1]{
	{\bf{#1}} 
}

\newcommand{\ffrac}[2]{\ensuremath{\frac{\displaystyle #1}{\displaystyle #2}}}

\begin{document}

\mainmatter  % start of an individual contribution
% first the title is needed
\title{Investigation of step-size adaptation methods for CMA-ES based on population midpoint fitness}

% a short form should be given in case it is too long for the running head
\titlerunning{Step-size adaptation for CMA-ES based on population midpoint fitness}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%

%lista i kolejnosc na niej do ustalenia
\author{Jaros\l aw Arabas\and 
Eryk Warchulski}
%
\authorrunning{J.Arabas \and E.Warchulski}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Warsaw University of Technology, Institute of Computer Science, Poland\\ 
\mailsa, \mailsb 
}


%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}

{\it Abstract to be compacted: JA}

Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is the basic algorithm of many top-performing methods that have been the winners of black box competitions, such as BBOB or CEC. The effectiveness of CMA-ES is occupied by a considerable computational effort that is needed to perform matrix operations, including the factorization of the inverse matrix that is needed to implement the cumulative step adaptation method (CSA) used to adapt the step-size multiplier $\sigma$.

Therefore, a line of research is aimed at simplification or substitution matrix operations. In effect, several alternative methods to adapt $\sigma$ have been   introduced in the earlier literature. 

In this paper we introduce and investigate three different new methods to adapt $\sigma$. All investigated methods are based on the comparison of fitness values between the population midpoint and the offspring. All methods reveal geometric convergence for the quadratic fitness function. 

In the first two methods, $\sigma$ is increased when the success ratio exceeds 1/5, and is decreased when the success ratio falls below 1/5. The success ratio in each iteration is defined as a fraction of the number of points generated in that iteration whose fitness value exceeds the fitness of the weighted mean (for the first method) or of the simple arithmetic mean (for the second method) of points from the previous iteration.

In the third version, $\sigma$ is increased when the fitness of the current iteration midpoint is located below a certain percentile of the fitness values in that population.

We performed tests to compare the quality of results obtained by CMA-ES coupled with several different rules to control $\sigma$. These rules included three investigated methods, the standard CSA rule, and the median success rule (MSR) which was introduced in the earlier literature. The tests were performed using the CEC'2017 benchmark set. 

According to the results of tests, CSA allows for the best performance of CMA-ES. Among other step-size control techniques, the third introduced method based on the percentile analysis of the midpoint yielded consistently better results than all other methods. 

\end{abstract}
\keywords{CMA-ES, 1/5 success rate rule, CSA}

\section{Convergence of CMA-ES with various step-size adaptations}
\subsection{Compared methods}

In this section we investigated rates of convergence and computing overhead of proposed methods. 
As mentioned in previous sections we considered five methods using different $\sigma$ adaptation rules:
  \begin{enumerate}
    \item CMA-ES-CSA
    \item CMA-ES-MSR
    \item CMA-ES-EXPTH
    \item CMA-ES-JA
    \item CMA-ES-QUANT
  \end{enumerate}

Each method used default settings for basic parameters suggested by authors execpt for 
population size $\lambda$. All methods generated $\lambda = 4N$ points in each iteration.
For CMA-ES-MSR to count $K_{succ}$ of better points in current population than $j$-th best point 
of the previous population we set $j$ as:

\begin{equation*}
j = 0.3\lambda.
\end{equation*}

\subsection{Fitness functions: linear, quadratic, gutter}

To illustrate and compare rates of convergence we used three different fitness landscapes i.e. linear (\ref{eq:1}), quadratic
(\ref{eq:2}) and gutter function (\ref{eq:3}).

\begin{align}
  f_{l}(\wec{x}) &= x_{1}  \label{eq:1} \\
  f_{q}(\wec{x}) &= \sum^{D}_{i = 1} x^{2}_{i} \label{eq:2} \\
  f_{g}(\wec{x}) &= x_{1} + \sum^{D}_{i = 2} x^{2}_{i}, \; \wec{x} \in \mathbb{R}^{D}  \label{eq:3}
\end{align}

To assess the rates of convergence, each function was treated in a minimalization manner and we recorded the fitness of the 
arthimetic mean and the best point in the population. \\
The curves depicted in the figures below for iteration $t$ show the fitness of the best point and midpoint in the $t$th generation. \\ 
For each problem and algorithm, the mean point of population is initialized as:
\begin{equation*}
  \wec{m}_{i} = [100, \dots, 100]^{D}.
\end{equation*}

\subsection{Computing overhead: comparison of the number of fitness evaluations without and with adaptation of sigma}



\subsection{Conclusions: convergence rate, computing overhead, benefits from using midpoint, search space dimension}
todo

\section{Benchmarking step-size adaptation methods on CEC'2013 and CEC'2017}
\subsection{Overview of CEC'2013 and CEC'2017}

We tested the performance of proposed methods using standardized sets of single objective optimization problems i.e. CEC2017 \cite{cec2017} and CEC2013 \cite{2013}. Both of them are based on the benchmark set created in 2005 \cite{cec2005}.

Each problem in set is defined as follows:

\begin{equation*}
  \min_{\wec{x} \in [-100, 100]^{D}}\; f(\wec{x})
\end{equation*}

and should be considered as a black-box problem. One knows only dimensionality $D$, evaluation budget and value of global optimum. Note that the number of benchmark functions and functions by itself in CEC2013 and CEC2017 vary. 

CEC2013 has 28 functions which are dived into three groups: unimodal, multimodal and composition functions.
Structure of CEC2017 is similar with but with the addition of hybrid as a fourth group of function. 
Hybrid and composition functions are composed of several multimodal functions which are defined in such a way that in various regions of the
domain the dynamics of the objective function is dominated by different components of that composition.
Thus, the optimization algorithm must be able to capture these regularities over the whole run.
Structures of benchmarks are presented in the table below (\ref{cec-table}).

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|l|l|l|}
\hline
Function type & CEC2017                 & \multicolumn{4}{c|}{CEC2013}                                 \\ \hline
Unimodal      & $f_1, \dots, f_3$       & \multicolumn{4}{c|}{$f_1, \dots, f_5$}                       \\ \hline
Multimodal    & $f_4, \dots, f_{10}$    & \multicolumn{4}{c|}{\multirow{2}{*}{$f_{6}, \dots, f_{20}$}} \\ \cline{1-2}
Hybrid        & $f_{11}, \dots, f_{20}$ & \multicolumn{4}{c|}{}                                        \\ \hline
Composition   & $f_{21}, \dots, f_{30}$ & \multicolumn{4}{c|}{$f_{21}, \dots, f_{28}$}                 \\ \hline
\end{tabular}
\caption{Summary of CEC2017 and CEC2013.}
\label{cec-table}
\end{table}

We evaluated each algorithm with default settings for CEC benchmarks i.e. 51 independent repetitions for function, $10^{4}\cdot D$ objective function evaluations and dimensionality $D = 10, 30, 50$.

\subsection{Presentation of results using ECDF curves}

Instead of using tabular form of results presentation suggested by authors of CEC competitions, we decided to use ECDF (empirical cumulative distribution function) curve.
ECDF curve was proposed by Hansen \cite{Hansen-ecdf} as a convenient form of results aggregation. A single curve defines the average dynamics of the algorithm.
\\
More formally, let us suppose that one of the tested algorithms in $k$-th repetition of $D$-dimensional benchmark function $f \in \mathcal{F}$ recorded in generation $t$ value $Q^{f}_{k, D, t}$ 
which is the difference between the best-so-far objective function value and the global optimum. \\
Next, consider mapping from $Q^{f}_{k, D, t}$ to unit interval $[0, 1$]:

\begin{equation}
  q^{f}_{k, D, t} = \ffrac{\log\left(\frac{Q^{f}_{k, D, t}}{m^{f}_{D}}\right)}{\log\left(\frac{M^{f}_{D}}{m^{f}_{D}}\right)}
\end{equation}

where $m^{f}_{D}$ and $M^{f}_{D}$ are respectively minimal and maximal values achieved among all tested algorithms, repetitions and generations for the given set of problems. \\

To obtain ECDF points for functions from $\mathcal{F}$ one has to aggregate above values as follows:

\begin{equation}
  q^{F}_{D, t} = \frac{1}{K \cdot |F|}\sum_{f \in F} \sum^{K}_{k = 1} q^{f}_{k, D, t}  
\end{equation}

where $K$ is the number of repetitions.\\
The curve is constructed by plotting values of $q^{F}_{D, t}$ against the set of fitness evaluation milestones based on fraction of given budget. 

\subsection{Results}

We set the fractions as:
\begin{equation*}
  \{0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}
\end{equation*}

\section{Conclusions}

1. which method is advisable
{\bf  JA+EW}

2. how much computing we can spare by avoiding matrix operations
{\bf  JA+EW}

3. future research directions
{\bf  JA+EW}

{\bf bibliografia do uzupelnienia: EW}

\bibliographystyle{splncs}
\bibliography{ppsnJA}

\end{document}
\begin{figure}[htbp]
\centering
\psfrag{GR}[Bc][Bc][1][0]{Layer thickness (nm)}
\psfrag{LA}[Bc][Bc][1][0]{Layer \#}
\vspace{-0.5cm}
a)\includegraphics[width=3.7cm]{BRAGGGr.ps}
b)\includegraphics[width=3.7cm]{CMGr.ps}
c)\includegraphics[width=3.7cm]{DCM0Gr.ps}
\vspace{-0.3cm}
\caption{Layers thickness of: a)Bragg mirror, b)single chirped Bragg mirror, c)$\mathrm{DCM}_0$}
\label{LAY_TH}
\end{figure} 

